{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ConfigFile.ipynb  ##import ConfigFile as cfg\n",
    "%run DB_Connection.ipynb ##import DB_Connection as dbc\n",
    "import pyodbc\n",
    "import copy\n",
    "import math\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from io import StringIO\n",
    "import ast \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Modelling and evaluation imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger SAAB (DEBUG)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a unique id with the help of current date timestamp\n",
    "st = datetime.fromtimestamp(time.time()).strftime('%d%m%Y%H%M%S')\n",
    "clientname=client_name['clientname']\n",
    "logging.basicConfig(filename=clientname+\"_\"+st+\".log\",level=logging.DEBUG,format=\"%(asctime)s:%(levelname)s:%(message)s\")\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "logging.getLogger(clientname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_timestamp = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function performs a stratified sampling on the input data frame \n",
    "    Input:\n",
    "        input_dataframe : on which the sampling needs to be performed\n",
    "    Output:\n",
    "        Returns the train data, train labels, test data and test labels\n",
    "'''\n",
    "def stratified_sampling(input_dataframe):\n",
    "    X = input_dataframe['Preprocessed Transcripts']\n",
    "    y = input_dataframe['call_type']\n",
    "    sss = StratifiedShuffleSplit(n_splits=10,test_size=0.2,random_state=32)\n",
    "    sss.get_n_splits(X, y)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    return  X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function performs a featre selection based on the chi2 scores to exclude less weightage score\n",
    "    Input:\n",
    "        X_train : train data\n",
    "        y_train : train labels\n",
    "        no_of_features : \n",
    "        threshold = chi2 square threshold value\n",
    "    Output:\n",
    "        Dataframe of the best features\n",
    "'''\n",
    "def feature_selection(X_train,y_train,no_of_features,chi_square_threshold):\n",
    "    # Create dummies for classes\n",
    "    y_train_dummies = pd.get_dummies(y_train)\n",
    "\n",
    "    # Convert to token counts\n",
    "    count_vec = CountVectorizer(ngram_range=(2,3),binary=True)\n",
    "    X_train_count = count_vec.fit_transform(X_train)\n",
    "    transformer = TfidfTransformer()\n",
    "    X_train_transformed = transformer.fit_transform(X_train_count)\n",
    "    \n",
    "    columns = list(y_train_dummies.columns)\n",
    "    features_final=[]\n",
    "    chi_probability = []\n",
    "    for col in columns:\n",
    "        chi_score = chi2(X_train_transformed, y_train_dummies[col])[0]\n",
    "        chi_prob = chi2(X_train_transformed, y_train_dummies[col])[1]\n",
    "        features = count_vec.get_feature_names()\n",
    "        chi_table = pd.DataFrame({'Features':features, 'Chisquare':chi_score , 'Chi_Square_Prob':chi_prob})\n",
    "        chi_table_cutoff = chi_table.loc[(chi_table[\"Chisquare\"] > chi_square_threshold)] \n",
    "        if len(chi_table_cutoff)>no_of_features:\n",
    "            chi_table = chi_table_cutoff.sort_values(by='Chisquare', ascending=False).head(no_of_features)\n",
    "        else:\n",
    "            chi_table = chi_table_cutoff\n",
    "        features_final.append(chi_table['Features'].tolist())\n",
    "        chi_probability.append(chi_table['Chisquare'].tolist())\n",
    "    features_all = [item for sublist in features_final for item in sublist]\n",
    "    chi_values_all = [item for sublist in chi_probability for item in sublist]\n",
    "    features_all_table = pd.DataFrame({'Features':features_all , 'Chisquare':chi_values_all})\n",
    "    features_best_chi = pd.DataFrame(features_all_table.groupby(['Features'], as_index=False, sort=False)['Chisquare'].max())\n",
    "    \n",
    "    return  features_best_chi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best k features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function identifies the best numebr features to be used\n",
    "    Input:\n",
    "        features_best_chi : best features by chi2 score\n",
    "        X_train : train data\n",
    "        y_train : train labels\n",
    "        X_test : test data\n",
    "        y_test : test labels\n",
    "        \n",
    "    Output:\n",
    "        Return the optimal number of features to consider\n",
    "'''\n",
    "def find_best_k_features(features_best_chi,X_train,y_train,X_test,y_test):\n",
    "    k_val = features_best_chi.shape[0]\n",
    "#     print((k_val))\n",
    "    scores_f1 = []\n",
    "    for k in range(1, int(k_val), 1):\n",
    "        features_table = features_best_chi.sort_values('Chisquare', ascending=False).head(k)\n",
    "        # Bi gram\n",
    "        model = Pipeline([('vect', CountVectorizer(vocabulary=features_table.Features,ngram_range=(2,3))),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf',  CalibratedClassifierCV(LinearSVC(penalty=\"l2\", dual=False, tol=1e-3),cv=KFold(n_splits=3))),])#LinearSVC(penalty=\"l2\", dual=False,tol=1e-3)\n",
    "        # Training the Model\n",
    "        model.fit(X_train, y_train)\n",
    "        # Scoring the Model\n",
    "        predicted = model.predict(X_test)\n",
    "        scores_f1.append(f1_score(y_test, predicted, average='weighted'))\n",
    "\n",
    "    kvals_list = list(range(1,int(k_val),1))\n",
    "\n",
    "    scores_f1_table = pd.DataFrame({'K_values':kvals_list, 'F1-Score':scores_f1})\n",
    "    kval_top = scores_f1_table.sort_values(by='F1-Score', ascending=False).head(n=10)\n",
    "    kval_optimum = int(kval_top.iloc[0,0])\n",
    "    \n",
    "    return kval_optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function trains the final model\n",
    "    Input:\n",
    "        features_table_chi_top : top best features selected\n",
    "        X_train : train data\n",
    "        y_train : train labels\n",
    "        \n",
    "    Output:\n",
    "        Returns the trained model\n",
    "'''\n",
    "def train_model(features_table_chi_top,X_train,y_train):\n",
    "    #Using calibrated classifier \n",
    "    model = Pipeline([('vect', CountVectorizer(vocabulary=features_table_chi_top.Features,ngram_range=(2,3))),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf',  CalibratedClassifierCV(LinearSVC(penalty=\"l2\", dual=False,tol=1e-3),cv=KFold(n_splits=3)))])\n",
    "\n",
    "    # Training the Model\n",
    "    trained_model = model.fit(X_train, y_train)\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to DW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Function to write the dataframe to DW\n",
    "  \n",
    "    Input :\n",
    "        call_types_df : dataframe that consists of labels of the call types assigned for each call\n",
    "            \n",
    "'''\n",
    "def write_df_to_dw(call_types_df):\n",
    "    # Writing to DW\n",
    "    today_timestamp = datetime.now()\n",
    "    server = mysql['server']\n",
    "    database = mysql['database']\n",
    "    username = mysql['username']\n",
    "    password = mysql['password']\n",
    "    conn = pyodbc.connect('DRIVER={ODBC Driver 13 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.setinputsizes([(pyodbc.SQL_INTEGER,), (pyodbc.SQL_INTEGER,),(pyodbc.SQL_INTEGER,),(pyodbc.SQL_WVARCHAR,20,0),(pyodbc.SQL_WVARCHAR,0), (pyodbc.SQL_TYPE_TIMESTAMP), (pyodbc.SQL_WVARCHAR,50,0)])\n",
    "    id_count = 0\n",
    "    for index,row in word_count_df.iterrows():\n",
    "        id_count += 1\n",
    "        cursor.execute(\"INSERT INTO [dbo].[SAAB_ML_WORDCOUNT_FT]([RESULT_ID], [CALL_ID], [DOMAIN_ID], [TRANSCRIPT_TYPE], [TRANSCRIPT], [CREATED_DATE], [CREATED_BY]) values (?, ?, ?, ?, ?, ?, ?)\", id_count, row['Call_ID'], row['Domain_ID'], row['Transcript_Type'], row['Transcript'],  today_timestamp, row['Created_By'])\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write model metrics to DW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Function to write the model metrics to DW\n",
    "  \n",
    "    Input :\n",
    "        call_types_df : dataframe that consists of labels of the call types assigned for each call\n",
    "            \n",
    "'''\n",
    "def write_metrics_df_to_dw(metrics_df):\n",
    "    # Writing to DW\n",
    "    today_timestamp = datetime.now()\n",
    "    server = mysql['server']\n",
    "    database = mysql['database']\n",
    "    username = mysql['username']\n",
    "    password = mysql['password']\n",
    "    conn = pyodbc.connect('DRIVER={ODBC Driver 13 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "    cursor = conn.cursor()\n",
    "#     cursor.setinputsizes([(pyodbc.SQL_INTEGER,), (pyodbc.SQL_INTEGER,),(pyodbc.SQL_INTEGER,),(pyodbc.SQL_WVARCHAR,20,0),(pyodbc.SQL_WVARCHAR,0), (pyodbc.SQL_TYPE_TIMESTAMP), (pyodbc.SQL_WVARCHAR,50,0)])\n",
    "    id_count = 0\n",
    "    for index,row in metrics_df.iterrows():\n",
    "#         print(row)\n",
    "#         master_id = fetch_data(\"select MAX(MASTER_ID) from [dbo].[SAAB_ML_MODEL_METRICS_FT]\")\n",
    "#         print(master_id)\n",
    "#         if master_id is not None:\n",
    "#             print(\"True\")\n",
    "#             id_count = fetch_data(\"select MAX(MASTER_ID) from [dbo].[SAAB_ML_MODEL_METRICS_FT]\") + 1\n",
    "#         else:\n",
    "#             print(\"False\")\n",
    "        id_count = 1\n",
    "        cursor.execute(\"INSERT INTO [dbo].[SAAB_ML_MODEL_METRICS_FT]([MASTER_ID], [MODEL_NAME], [METRIC_NAME], [VALIDATION_EVAL_METRIC], [CREATED_DATE], [CREATED_BY]) values (?, ?, ?, ?, ?, ?)\", id_count, row['Model_Name'], row['Metric_Name'], row[\"Validation_Eval_Metric\"], today_timestamp, row['Created_By'])\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.debug(\"Starting....\")\n",
    "    # Reading data from local system and azure data\n",
    "    try:\n",
    "        domain_id = 101\n",
    "        created_by = \"MDXC\"\n",
    "        top_level_container = \"preprocessed-transcripts\"\n",
    "\n",
    "        manual_label_data = pd.read_excel(\"CallTypeLabels.xlsx\",sheet_name=\"Sheet1\")\n",
    "        blob_file = fetch_data_from_blob(top_level_container)\n",
    "        preprocessed_data = pd.read_csv(StringIO(blob_file))\n",
    "        call_name_data = fetch_data(\"select call_ID,file_name from [dbo].[Fact_Audio_Processed];\")\n",
    "\n",
    "    except Exception as err:\n",
    "            logging.error(\"Error occured while connecting to storage account :\"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "            sys.exit(1)\n",
    "    try:\n",
    "        # Merging data for input into training the model\n",
    "        logging.debug(\"Merging data in progress....\")\n",
    "        merged_label_data = pd.merge(manual_label_data,call_name_data,left_on=\"CallName\",right_on=\"file_name\")\n",
    "        merged_label_data = merged_label_data[['call_ID','call_type']]\n",
    "        merged_label_data = pd.merge(merged_label_data,preprocessed_data,left_on=\"call_ID\",right_on=\"Call_ID\")\n",
    "        merged_label_data = merged_label_data[['call_ID','call_type','Preprocessed Transcripts']]\n",
    "        merged_label_data['Preprocessed Transcripts'] = merged_label_data['Preprocessed Transcripts'].apply(lambda x: \" \".join(ast.literal_eval(x.replace(\"_\",\" \"))))\n",
    "        merged_label_data\n",
    "        \n",
    "        #Stratified Sampling\n",
    "        logging.debug(\"Stratified sampling in progress....\")\n",
    "        X_train, y_train, X_test, y_test = stratified_sampling(merged_label_data)\n",
    "\n",
    "        # Feature selection\n",
    "        logging.debug(\"Finding best features in progress....\")\n",
    "        features_best_chi = feature_selection(X_train,y_train,1000,0)\n",
    "\n",
    "        # Finding best K-features\n",
    "        kval_optimum = find_best_k_features(features_best_chi,X_train, y_train, X_test, y_test)\n",
    "\n",
    "        #Training data with optimal features\n",
    "        logging.debug(\"Training in progress....\")\n",
    "        features_table_chi_top = features_best_chi.sort_values('Chisquare', ascending=False).head(kval_optimum)\n",
    "        final_trained_model = train_model(features_table_chi_top,X_train, y_train)\n",
    "        \n",
    "        # Save model to disk\n",
    "        filename = 'finalized_call_type_model.sav'\n",
    "        pickle.dump(final_trained_model, open(filename, 'wb'))\n",
    "        \n",
    "        # Predict on test set\n",
    "        logging.debug(\"Prediction and scoring in progress....\")\n",
    "        predicted_labels = final_trained_model.predict(X_test)\n",
    "        prec_recall_fscore_support = precision_recall_fscore_support(y_test, predicted_labels)\n",
    "        accuracy = accuracy_score(y_test,predicted_labels)\n",
    "        print(accuracy)\n",
    "    \n",
    "        \n",
    "        # Write model metrics to table\n",
    "        logging.debug(\"Writing model metrics to DW in progress...\")\n",
    "#         metric_df = pd.DataFrame([[\"CallTypes_Classification_M1\",\"Accuracy\",accuracy,\"MDXC\"]], columns=['Model_Name','Metric_Name',\"Validation_Eval_Metric\",\"Created_By\"])\n",
    "#         write_metrics_df_to_dw(metric_df) \n",
    "    \n",
    "    except Exception as err:     \n",
    "        logging.error(\"Error occured : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        logging.debug(\"End of call types classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG - Starting....\n",
      "DEBUG - Starting new HTTPS connection (1): saabstorageresource.blob.core.windows.net:443\n",
      "DEBUG - https://saabstorageresource.blob.core.windows.net:443 \"GET /preprocessed-transcripts?restype=container&comp=list HTTP/1.1\" 200 None\n",
      "DEBUG - https://saabstorageresource.blob.core.windows.net:443 \"GET /preprocessed-transcripts/Preprocessed_Transcripts_29012020161128.csv HTTP/1.1\" 206 170282\n",
      "DEBUG - Merging data in progress....\n",
      "DEBUG - Stratified sampling in progress....\n",
      "DEBUG - Finding best features in progress....\n",
      "DEBUG - Training in progress....\n",
      "DEBUG - Prediction and scoring in progress....\n",
      "DEBUG - Writing model metrics to DW in progress...\n",
      "DEBUG - End of call types classification\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6875\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the labelled data to DW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG - Starting new HTTPS connection (1): saabstorageresource.blob.core.windows.net:443\n",
      "DEBUG - https://saabstorageresource.blob.core.windows.net:443 \"GET /preprocessed-transcripts?restype=container&comp=list HTTP/1.1\" 200 None\n",
      "DEBUG - https://saabstorageresource.blob.core.windows.net:443 \"GET /preprocessed-transcripts/Preprocessed_Transcripts_29012020161128.csv HTTP/1.1\" 206 170282\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>call_ID</th>\n",
       "      <th>call_type</th>\n",
       "      <th>Preprocessed Transcripts</th>\n",
       "      <th>Call_Type_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136043333</td>\n",
       "      <td>Complaint</td>\n",
       "      <td>hello good help want term insurance policy ben...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1444129832</td>\n",
       "      <td>Enquiry/Support</td>\n",
       "      <td>hello help sorry want convert term life policy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>288021220</td>\n",
       "      <td>Enquiry/Support</td>\n",
       "      <td>hello good evening help actually premium month...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>800562374</td>\n",
       "      <td>Enquiry/Support</td>\n",
       "      <td>hello good help height dumb policy year expire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2146526476</td>\n",
       "      <td>Complaint</td>\n",
       "      <td>welcome life help high angel policy number que...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>61876574</td>\n",
       "      <td>Enquiry/Support</td>\n",
       "      <td>good customer care help today want unit link p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1344343262</td>\n",
       "      <td>Complaint</td>\n",
       "      <td>life help today face problem policy unit link ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1791382475</td>\n",
       "      <td>Complaint</td>\n",
       "      <td>life customer care help today actually policy ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1692229584</td>\n",
       "      <td>Complaint</td>\n",
       "      <td>good life customer care help today actually pr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1884492469</td>\n",
       "      <td>Complaint</td>\n",
       "      <td>hello hdfcla help want plus plan want know opt...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       call_ID        call_type  \\\n",
       "0    136043333        Complaint   \n",
       "1   1444129832  Enquiry/Support   \n",
       "2    288021220  Enquiry/Support   \n",
       "3    800562374  Enquiry/Support   \n",
       "4   2146526476        Complaint   \n",
       "..         ...              ...   \n",
       "72    61876574  Enquiry/Support   \n",
       "73  1344343262        Complaint   \n",
       "74  1791382475        Complaint   \n",
       "75  1692229584        Complaint   \n",
       "76  1884492469        Complaint   \n",
       "\n",
       "                             Preprocessed Transcripts  Call_Type_ID  \n",
       "0   hello good help want term insurance policy ben...             2  \n",
       "1   hello help sorry want convert term life policy...             1  \n",
       "2   hello good evening help actually premium month...             1  \n",
       "3   hello good help height dumb policy year expire...             1  \n",
       "4   welcome life help high angel policy number que...             2  \n",
       "..                                                ...           ...  \n",
       "72  good customer care help today want unit link p...             1  \n",
       "73  life help today face problem policy unit link ...             2  \n",
       "74  life customer care help today actually policy ...             2  \n",
       "75  good life customer care help today actually pr...             2  \n",
       "76  hello hdfcla help want plus plan want know opt...             2  \n",
       "\n",
       "[77 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Reading data from local system and azure data\n",
    "domain_id = 101\n",
    "created_by = \"MDXC\"\n",
    "top_level_container = \"preprocessed-transcripts\"\n",
    "\n",
    "manual_label_data = pd.read_excel(\"CallTypeLabels.xlsx\",sheet_name=\"Sheet1\")\n",
    "blob_file = fetch_data_from_blob(top_level_container)\n",
    "preprocessed_data = pd.read_csv(StringIO(blob_file))\n",
    "call_name_data = fetch_data(\"select call_ID,file_name from [dbo].[Fact_Audio_Processed];\")\n",
    "\n",
    "merged_label_data = pd.merge(manual_label_data,call_name_data,left_on=\"CallName\",right_on=\"file_name\")\n",
    "merged_label_data = merged_label_data[['call_ID','call_type']]\n",
    "merged_label_data = pd.merge(merged_label_data,preprocessed_data,left_on=\"call_ID\",right_on=\"Call_ID\")\n",
    "merged_label_data = merged_label_data[['call_ID','call_type','Preprocessed Transcripts']]\n",
    "merged_label_data['Preprocessed Transcripts'] = merged_label_data['Preprocessed Transcripts'].apply(lambda x: \" \".join(ast.literal_eval(x.replace(\"_\",\" \"))))\n",
    "merged_label_data['Call_Type_ID'] = np.where(merged_label_data['call_type']==\"Enquiry/Support\",1,2)\n",
    "merged_label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = 'saab-server-resource.database.windows.net'\n",
    "database = 'SAAB_DW_Resource'\n",
    "username = 'saabadmin'\n",
    "password = 'p@$$w0rd'\n",
    "conn = pyodbc.connect('DRIVER={ODBC Driver 13 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = conn.cursor()\n",
    "id_count = 0\n",
    "for index,row in merged_label_data.iterrows():\n",
    "    id_count += 1\n",
    "    cursor.execute(\"INSERT INTO [dbo].[SAAB_ML_CALLTYPES_FT]([MASTER_ID],[DOMAIN_ID],[CALLTYPE_ID],[CALL_ID],[CREATED_DATE],[CREATED_BY]) values (?, ?, ?, ?, ?, ?)\", id_count, 101,row['Call_Type_ID'], row['call_ID'], today_timestamp,'MDXC') \n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to call types master table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>master_id</th>\n",
       "      <th>calltype_id</th>\n",
       "      <th>calltype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Enquiry/Support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Complaint</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   master_id  calltype_id         calltype\n",
       "0          1            1  Enquiry/Support\n",
       "1          2            2        Complaint"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def write_call_types_dm_table():\n",
    "master_df = pd.DataFrame()\n",
    "master_df['master_id'] = [1,2]\n",
    "# master_df['domain_id'] = [101,101]\n",
    "master_df['calltype_id'] = [1,2]\n",
    "master_df['calltype'] = ['Enquiry/Support','Complaint']\n",
    "#     master_df['keywords'] = ['{\"Section_1\":[\"good morning\", \"good afternoon\"],\"Section_2\":[\"welcome to\", \"you are through to\",\"thank you for calling\"],\"Section_3\":[\"how can i help\", \"how may i help\", \"what can I assist\", \"what may i assist\"]}','{\"Section_1\":[\"thanks for calling\",\"thank you for calling\",\"have a nice day\",\"have a good day\",\"thank you for contacting\", \"have a great day\", \"is there anything else\"]}']\n",
    "# master_df['created_date'] = [today_timestamp,today_timestamp]\n",
    "# master_df['created_by'] = [\"MDXC\",\"MDXC\"]\n",
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = 'saab-server-resource.database.windows.net'\n",
    "database = 'SAAB_DW_Resource'\n",
    "username = 'saabadmin'\n",
    "password = 'p@$$w0rd'\n",
    "conn = pyodbc.connect('DRIVER={ODBC Driver 13 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "cursor = conn.cursor()\n",
    "for index,row in master_df.iterrows():\n",
    "    cursor.execute(\"INSERT INTO [dbo].[SAAB_ML_MASTER_CALLTYPES_DM]([RESULT_ID],[DOMAIN_ID],[CALLTYPE_ID],[CALLTYPE],[CREATED_DATE],[CREATED_BY]) values (?, ?, ?, ?, ?, ?)\", row['master_id'], 101,row['calltype_id'], row['calltype'], today_timestamp,'MDXC') \n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
