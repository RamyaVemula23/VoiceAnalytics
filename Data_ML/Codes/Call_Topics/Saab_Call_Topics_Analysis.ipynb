{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ConfigFile as cfg\n",
    "import DB_Connection as dbc\n",
    "import gensim\n",
    "import logging\n",
    "import pyodbc\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from azure.storage.blob import ContainerClient,BlobClient,BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the random see and getting the current working directory\n",
    "np.random.seed(1)\n",
    "local_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger SAAB (DEBUG)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a unique id with the help of current date timestamp\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%d%m%Y%H%M%S')\n",
    "clientname=cfg.client_name['clientname']\n",
    "logging.basicConfig(\n",
    "    filename=clientname+\"_\"+st+\".log\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    ")\n",
    "logging.getLogger(clientname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching the preprocessed file from azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_file_from_azure():\n",
    "    service = ContainerClient(account_url=cfg.azure_details['account_url'],container_name=cfg.azure_details['storage_preprocessed_transcripts'], credential=cfg.azure_details['azure_storage_account_key'])\n",
    "    blob_list = service.list_blobs()\n",
    "    blob_name=''\n",
    "    for blob in blob_list:\n",
    "        blob_name = blob.name\n",
    "    \n",
    "    # Create the BlobServiceClient object which will be used to create a container client\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(cfg.azure_details['account_connection_string'])\n",
    "    local_file_name = blob_name\n",
    "    full_path_to_file = os.path.join(local_path, local_file_name)\n",
    "    # Create a blob client using the local file name as the name for the blob\n",
    "    container_name=\"preprocessed-transcripts\"\n",
    "    blob_client = blob_service_client.get_blob_client(container=cfg.azure_details['storage_preprocessed_transcripts'], blob=local_file_name)\n",
    "    download_file_path = os.path.join(local_path, local_file_name)\n",
    "    print(\"\\nDownloading blob to \\n\\t\" + download_file_path)\n",
    "\n",
    "    with open(download_file_path, \"wb\") as download_file:\n",
    "        download_file.write(blob_client.download_blob().readall())\n",
    "    \n",
    "    return local_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the total number of topics that needs to be created for this domain of customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_from_DB():\n",
    "    #df=pd.read_csv('Master.csv')\n",
    "    df=dbc.fetch_data(\"select calltopic_id,calltopics from [SAAB_ML_MASTER_CALLTOPICS_DM] where [DOMAIN_ID]=\"+cfg.client_details['Domain_ID'] +\"order by calltopic_id\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the topic modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LDA_Model(preprocessed_data,Number_Of_Topics):\n",
    "    # Getting trigram  \n",
    "    vectorizer = CountVectorizer(ngram_range = (3,3)) \n",
    "    X1=vectorizer.fit_transform(preprocessed_data['Preprocessed Transcripts'])\n",
    "    trigrams=[]\n",
    "    for text in preprocessed_data['Preprocessed Transcripts']:\n",
    "        text=[text,]\n",
    "        X1=(vectorizer.fit_transform(text))\n",
    "        trigrams.append(vectorizer.get_feature_names()) \n",
    "    num_topics=Number_Of_Topics\n",
    "    preprocessed_data['trigram_Transcript']=trigrams\n",
    "    dictionary = gensim.corpora.Dictionary(preprocessed_data['trigram_Transcript'])\n",
    "    \n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_data['trigram_Transcript']]\n",
    "    lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                       num_topics = num_topics, \n",
    "                                       id2word = dictionary,                                    \n",
    "                                       passes =10,\n",
    "                                       workers = 1,random_state = np.random.seed(111))\n",
    "    \n",
    "    coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, texts=preprocessed_data['trigram_Transcript'], dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    #print('\\nCoherence Score: ', coherence_lda)\n",
    "    \n",
    "    return lda_model,lda_model.log_perplexity(bow_corpus),coherence_lda,bow_corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the topics for the calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topics(bow_corpus,lda_model,num_topics):\n",
    "    #predicting on the corpus\n",
    "    doc_lda = lda_model[bow_corpus]\n",
    "    results=[]\n",
    "    for topic in doc_lda:\n",
    "        results.append(topic)\n",
    "    \n",
    "    #Getting the topics into a dataframe as column name and scores for each topic into cell per call\n",
    "    cols=[]\n",
    "    for i in range(0,num_topics):\n",
    "        cols.append(\"Topic_\"+str(i+1))\n",
    "    df_Topics=pd.DataFrame(columns=cols)\n",
    "    \n",
    "    for index in range(len(results)):\n",
    "        col_val=''\n",
    "        for item in (results[index]):\n",
    "            df_Topics.set_value(index, \"Topic_\"+str(item[0]+1), str(item[1]))\n",
    "    \n",
    "    return df_Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final dataframe to be used to insert data into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def staging_final_dataframe(preprocessed_data,df_Topics,df_master_topics):\n",
    "    df_Topics.columns=df_master_topics.calltopics\n",
    "    preprocessed_data=pd.concat([preprocessed_data, df_Topics], axis=1)\n",
    "    preprocessed_data.drop(['Raw Transcripts', 'Preprocessed Transcripts'], axis=1, inplace=True)\n",
    "    value_vars=df_master_topics.calltopics.to_list()\n",
    "    df_result=pd.melt(preprocessed_data, id_vars =['Call_ID'], value_vars =value_vars ,\n",
    "              var_name ='CallTopic', value_name ='CALLTOPIC_SCORE').dropna().sort_values('Call_ID')\n",
    "    final_df=pd.merge(df_result, df_master_topics, how ='outer', left_on='CallTopic',right_on='calltopics')     \n",
    "    \n",
    "    final_df.sort_values(by=['Call_ID'],inplace=True)\n",
    "    final_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    final_df[\"Domain_ID\"]=int(cfg.client_details['Domain_ID'])\n",
    "    final_df['CREATED_BY']=cfg.client_details['UserName']\n",
    "    final_df['RESULT_ID']=final_df.index+1\n",
    "    final_df.drop(['CallTopic', 'calltopics'], axis=1, inplace=True)\n",
    "    \n",
    "    column_titles = ['RESULT_ID','Call_ID','Domain_ID','calltopic_id','CALLTOPIC_SCORE','CREATED_BY']\n",
    "    final_df.reindex(columns=column_titles)\n",
    "    \n",
    "    return final_df   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inserting the topic id and topic id score for each call into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_into_database(df):\n",
    "    conn = pyodbc.connect('DRIVER='+cfg.mysql['driver']+';SERVER='+cfg.mysql['server']+';PORT='+cfg.mysql['PORT']+';DATABASE='+cfg.mysql['database']+';UID='+cfg.mysql['username']+';PWD='+ cfg.mysql['password'])\n",
    "    cursor = conn.cursor()\n",
    "    for index,row in df.iterrows():\n",
    "        cursor.execute(\"INSERT INTO [dbo].[SAAB_ML_CALLTOPICS_FT]([RESULT_ID],[CALL_ID],[DOMAIN_ID],[CALLTOPIC_ID],[CALLTOPIC_SCORE],[CREATED_DATE],[CREATED_BY])VALUES(?, ?, ?, ?, ?, ?, ?)\", row['RESULT_ID'],row['Call_ID'],row['Domain_ID'],float(row['calltopic_id']),row['CALLTOPIC_SCORE'],datetime.datetime.now(),row['CREATED_BY'])\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    logging.debug(\"Download preprocessed transcripts file from azure storage\")\n",
    "    try:\n",
    "        #download preprocessed data file from azure blob storage\n",
    "        local_file_name=get_preprocessed_file_from_azure()\n",
    "        logging.debug(\"Downloaded preprocessed transcripts file from azure storage\")\n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured while fetching file from azure storage :\"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logging.debug(\"Reading the downloaded preprocessed transcripts file into a dataframe\")\n",
    "    try:\n",
    "        #storing the downloaded file into a dataframe\n",
    "        preprocessed_data=pd.read_csv(local_file_name)\n",
    "        logging.debug(\"Dataframe is populated with preprocessed transcripts\")\n",
    "        \n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logging.debug(\"Fetching the topics to be created for the dataset from DB\")\n",
    "    try:\n",
    "        #Fetch the topics to be created from database\n",
    "        df_master_topics=get_topics_from_DB()\n",
    "        logging.debug(\"Topics fetched from DB\")\n",
    "    except Exception as err:\n",
    "        if \"Could not open a connection to SQL Server\" in str(err):\n",
    "            logging.error(\"Could not connect to data warehouse : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        else:\n",
    "            logging.error(\"Error occured with database :\"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logging.debug(\"Building the LDA model and getting model metrics\")\n",
    "    try:\n",
    "        #Get the number of topics that needs to be created as part of this analysis\n",
    "        Number_Of_Topics=len(df_master_topics)\n",
    "        \n",
    "        #Build the LDA model\n",
    "        lda_model,perplexity_score,coherence_lda,bow_corpus=train_LDA_Model(preprocessed_data,Number_Of_Topics)\n",
    "        logging.debug(\"LDA model is trained\")\n",
    "    \n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    \n",
    "    logging.debug(\"Topics Generated \")\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        logging.debug(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    \n",
    "    logging.debug(\"Predicting the topics\")\n",
    "    try:\n",
    "        df_topics=predict_topics(bow_corpus,lda_model,Number_Of_Topics)\n",
    "        logging.debug(\"Prediction of topics is completed for each call\")\n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logging.debug(\"Staging data to be inserted into database\")\n",
    "    try:\n",
    "        final_df = staging_final_dataframe(preprocessed_data,df_topics,df_master_topics)\n",
    "        logging.debug(\"Final data frame is ready to be inserted into database\")\n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logging.debug(\"Push data into database\")\n",
    "    try:\n",
    "        insert_data_into_database(final_df)\n",
    "        logging.debug(\"Data inserted into database\")\n",
    "    except Exception as err:\n",
    "        if \"Could not open a connection to SQL Server\" in str(err):\n",
    "            logging.error(\"Could not connect to data warehouse : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        else:\n",
    "            logging.error(\"Error occured with database :\"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        logging.debug(\"End of call topics\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
