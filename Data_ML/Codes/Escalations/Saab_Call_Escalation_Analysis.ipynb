{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ConfigFile as cfg\n",
    "import DB_Connection as dbc\n",
    "import gensim\n",
    "import logging\n",
    "import pyodbc\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from azure.storage.blob import ContainerClient,BlobClient,BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the random see and getting the current working directory\n",
    "np.random.seed(1)\n",
    "local_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger SAAB (DEBUG)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a unique id with the help of current date timestamp\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%d%m%Y%H%M%S')\n",
    "clientname=cfg.client_name['clientname']\n",
    "logging.basicConfig(\n",
    "    filename=clientname+\"_\"+st+\".log\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    ")\n",
    "logging.getLogger(clientname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download preprocessed file from azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_file_from_azure():\n",
    "    service = ContainerClient(account_url=cfg.azure_details['account_url'],container_name=cfg.azure_details['storage_preprocessed_transcripts'], credential=cfg.azure_details['azure_storage_account_key'])\n",
    "    blob_list = service.list_blobs()\n",
    "    blob_name=''\n",
    "    for blob in blob_list:\n",
    "        blob_name = blob.name\n",
    "    \n",
    "    # Create the BlobServiceClient object which will be used to create a container client\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(cfg.azure_details['account_connection_string'])\n",
    "    local_file_name = blob_name\n",
    "    full_path_to_file = os.path.join(local_path, local_file_name)\n",
    "    # Create a blob client using the local file name as the name for the blob\n",
    "    container_name=\"preprocessed-transcripts\"\n",
    "    blob_client = blob_service_client.get_blob_client(container=cfg.azure_details['storage_preprocessed_transcripts'], blob=local_file_name)\n",
    "    download_file_path = os.path.join(local_path, local_file_name)\n",
    "\n",
    "    with open(download_file_path, \"wb\") as download_file:\n",
    "        download_file.write(blob_client.download_blob().readall())\n",
    "    \n",
    "    return local_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the keywords from DB for escalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_from_DB():\n",
    "    #df=pd.read_csv('Master.csv')\n",
    "    df=dbc.fetch_data(\"select [MASTER_ID],[ESCALATION_KEYWORDS],[THREAT_KEYWORDS]  from [SAAB_ML_MASTER_ESCALATION_THREAT_DM] where [DOMAIN_ID]=\"+cfg.client_details['Domain_ID'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning the escalation flag for each call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_escalation_in_calls(preprocessed_data,df_master_keywords):\n",
    "    escalation_keywords=df_master_keywords.iloc[0]['ESCALATION_KEYWORDS'].split(',')\n",
    "    preprocessed_data['HasEscalation'] = 0\n",
    "    escalationflag = \"\"\n",
    "    HasEscalation=[]\n",
    "    for i in range(len(preprocessed_data)):# df.iterrows():\n",
    "        Raw_Transcripts = preprocessed_data.iloc[i]['Raw Transcripts']\n",
    "        Preprocessed_Transcripts = preprocessed_data.iloc[i]['Preprocessed Transcripts']\n",
    "        for escalation in escalation_keywords:\n",
    "            if escalation in Raw_Transcripts or escalation in Preprocessed_Transcripts:\n",
    "                HasEscalation.append(i)\n",
    "                preprocessed_data.set_value(i,'HasEscalation',1)\n",
    "                break;\n",
    "    \n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the escalation flag into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_into_database(df):\n",
    "    conn = pyodbc.connect('DRIVER='+cfg.mysql['driver']+';SERVER='+cfg.mysql['server']+';PORT='+cfg.mysql['PORT']+';DATABASE='+cfg.mysql['database']+';UID='+cfg.mysql['username']+';PWD='+ cfg.mysql['password'])\n",
    "    cursor = conn.cursor()\n",
    "    id_count=0\n",
    "    for index,row in df.iterrows():\n",
    "        id_count+=1\n",
    "        cursor.execute(\"INSERT INTO [dbo].[SAAB_ML_ADDITIONAL_RESULT_FT]([RESULT_ID],[DOMAIN_ID],[CALL_ID],[HAS_ESCALATION],[CREATED_DATE],[CREATED_BY])VALUES(?, ?, ?, ?, ?, ?)\", str(id_count),int(cfg.client_details['Domain_ID']),int(row['Call_ID']) ,bool(row['HasEscalation']),datetime.datetime.now(),cfg.client_details['UserName'])\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.debug(\"Download preprocessed transcripts file from azure storage\")\n",
    "    try:\n",
    "        #download preprocessed data file from azure blob storage\n",
    "        local_file_name=get_preprocessed_file_from_azure()\n",
    "        logging.debug(\"Downloaded preprocessed transcripts file from azure storage\")\n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured while fetching file from azure storage :\"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logging.debug(\"Reading the downloaded preprocessed transcripts file into a dataframe\")\n",
    "    try:\n",
    "        #storing the downloaded file into a dataframe\n",
    "        preprocessed_data=pd.read_csv(local_file_name)\n",
    "        logging.debug(\"Dataframe is populated with preprocessed transcripts\")\n",
    "\n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "        \n",
    "    logging.debug(\"Fetching the keywords from DB\")\n",
    "    try:\n",
    "        #Fetch the keywords from database\n",
    "        df_master_keywords=get_keywords_from_DB()\n",
    "        logging.debug(\"Keywords fetched from DB\")\n",
    "    except Exception as err:\n",
    "        if \"Could not open a connection to SQL Server\" in str(err):\n",
    "            logging.error(\"Could not connect to data warehouse : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        else:\n",
    "            logging.error(\"Error occured with database :\"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logging.debug(\"Fetching the presence of escalation keywords per call\")\n",
    "    try:\n",
    "        #Verify the keywords presence in the calls\n",
    "        final_df = determine_escalation_in_calls(preprocessed_data,df_master_keywords)\n",
    "        logging.debug(\"Escalation calls determined\")\n",
    "        \n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "        \n",
    "    logging.debug(\"Push data into database\")\n",
    "    try:\n",
    "        insert_data_into_database(final_df)\n",
    "        logging.debug(\"Data inserted into database\")\n",
    "    except Exception as err:\n",
    "        if \"Could not open a connection to SQL Server\" in str(err):\n",
    "            logging.error(\"Could not connect to data warehouse : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        else:\n",
    "            logging.error(\"Error occured with database :\"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        logging.debug(\"End of call escalation analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stripathy32\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
