{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ConfigFile as cfg\n",
    "import DB_Connection as dbc\n",
    "import gensim\n",
    "import logging\n",
    "import pyodbc\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from azure.storage.blob import ContainerClient,BlobClient,BlobServiceClient\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient\n",
    "from msrest.authentication import CognitiveServicesCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the random see and getting the current working directory\n",
    "np.random.seed(1)\n",
    "local_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger SAAB (DEBUG)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a unique id with the help of current date timestamp\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%d%m%Y%H%M%S')\n",
    "clientname=cfg.client_name['clientname']\n",
    "logging.basicConfig(\n",
    "    filename=clientname+\"_\"+st+\".log\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    ")\n",
    "logging.getLogger(clientname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching the preprocessed file from azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_file_from_azure():\n",
    "    service = ContainerClient(account_url=cfg.azure_details['account_url'],container_name=cfg.azure_details['storage_preprocessed_transcripts'], credential=cfg.azure_details['azure_storage_account_key'])\n",
    "    blob_list = service.list_blobs()\n",
    "    blob_name=''\n",
    "    for blob in blob_list:\n",
    "        blob_name = blob.name\n",
    "    \n",
    "    # Create the BlobServiceClient object which will be used to create a container client\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(cfg.azure_details['account_connection_string'])\n",
    "    local_file_name = blob_name\n",
    "    full_path_to_file = os.path.join(local_path, local_file_name)\n",
    "    # Create a blob client using the local file name as the name for the blob\n",
    "    container_name=\"preprocessed-transcripts\"\n",
    "    blob_client = blob_service_client.get_blob_client(container=cfg.azure_details['storage_preprocessed_transcripts'], blob=local_file_name)\n",
    "    download_file_path = os.path.join(local_path, local_file_name)\n",
    "\n",
    "    with open(download_file_path, \"wb\") as download_file:\n",
    "        download_file.write(blob_client.download_blob().readall())\n",
    "    \n",
    "    return local_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the sentiment labels and ranges from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_label_from_DB():\n",
    "    #df=pd.read_csv('Master.csv')\n",
    "    df=dbc.fetch_data(\"select SENTIMENT_ID,SENTIMENT_TYPES,SENTIMENT_RANGE from [SAAB_ML_MASTER_SENTIMENTS_DM] where [DOMAIN_ID]=\"+cfg.client_details['Domain_ID'] +\"order by SENTIMENT_ID\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the credential and end point for the text analytics API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticateClient():\n",
    "    subscription_key=cfg.azure_details['text_API_subscription_key']\n",
    "    endpoint=cfg.azure_details['text_API_endpoint']\n",
    "    credentials = CognitiveServicesCredentials(subscription_key)\n",
    "    text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credentials=credentials)\n",
    "    return text_analytics_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the sentiment score from azure text api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score_from_Azure(df_staging):\n",
    "    df_staging[\"Language\"]=cfg.client_details['call_language1']\n",
    "    df_staging['text']=df_staging['Raw Transcripts'].replace(regex=True,to_replace=r'[^A-Za-z0-9]',value=r' ')\n",
    "    \n",
    "    df_staging['id'] = range(1, len(df_staging) + 1)\n",
    "    \n",
    "    df_api=df_staging[['id','Language', 'text']]\n",
    "    out_dict=df_api.to_dict('records')\n",
    "    df_result=pd.DataFrame(columns={'Call_ID','ID_Doc','Sentiment_Score'})\n",
    "    id_returned=[]\n",
    "    Sentiment_Score=[]\n",
    "    df_result['Call_ID']=df_staging['Call_ID']\n",
    "    \n",
    "    client = authenticateClient()\n",
    "    response = client.sentiment(documents=out_dict)\n",
    "    for document in response.documents:\n",
    "        id_returned.append(document.id)\n",
    "        Sentiment_Score.append(format(document.score))\n",
    "        \n",
    "    \n",
    "    df_result['ID_Doc']=id_returned\n",
    "    df_result['Sentiment_Score']=Sentiment_Score\n",
    "\n",
    "    return df_result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Staging the final data frame for sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def staging_final_dataframe(preprocessed_data,df_sentiment_Score,df_master_sentiments):\n",
    "    \n",
    "    sentiment_id=[]\n",
    "    for index,row in df_sentiment_Score.iterrows():\n",
    "        \n",
    "        if float(row['Sentiment_Score']) >=  df_master_sentiments['SENTIMENT_RANGE'][0]:\n",
    "            sentiment_id.append(df_master_sentiments['SENTIMENT_ID'][0])\n",
    "        elif float(row['Sentiment_Score']) >=  df_master_sentiments['SENTIMENT_RANGE'][1] and float(row['Sentiment_Score']) <  df_master_sentiments['SENTIMENT_RANGE'][0]:\n",
    "            sentiment_id.append(df_master_sentiments['SENTIMENT_ID'][1])\n",
    "        elif float(row['Sentiment_Score']) >=  df_master_sentiments['SENTIMENT_RANGE'][2] and float(row['Sentiment_Score']) <  df_master_sentiments['SENTIMENT_RANGE'][1]:\n",
    "            sentiment_id.append(df_master_sentiments['SENTIMENT_ID'][2])\n",
    "    \n",
    "    df_sentiment_Score['sentiment_id']=sentiment_id\n",
    "\n",
    "    final_df=pd.merge(df_sentiment_Score, preprocessed_data, how ='outer', left_on='Call_ID',right_on='Call_ID')\n",
    "    result_df=pd.DataFrame()\n",
    "    result_df['Result_Id'] = final_df['id']\n",
    "    result_df['CALL_ID'] = final_df['Call_ID']\n",
    "    result_df['DOMAIN_ID'] = int(cfg.client_details['Domain_ID'])\n",
    "    result_df['SENTIMENT_ID'] = final_df['sentiment_id']\n",
    "    result_df['SENTIMENT_SCORE'] = final_df['Sentiment_Score']\n",
    "    result_df['CREATED_DATE'] = datetime.datetime.now()\n",
    "    result_df['CREATED_BY'] = cfg.client_details['UserName']\n",
    "\n",
    "    return result_df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the sentiment score into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_into_database(df):\n",
    "    conn = pyodbc.connect('DRIVER='+cfg.mysql['driver']+';SERVER='+cfg.mysql['server']+';PORT='+cfg.mysql['PORT']+';DATABASE='+cfg.mysql['database']+';UID='+cfg.mysql['username']+';PWD='+ cfg.mysql['password'])\n",
    "    cursor = conn.cursor()\n",
    "    for index,row in df.iterrows():\n",
    "        cursor.execute(\"INSERT INTO [dbo].[SAAB_ML_SENTIMENTS_FT]([RESULT_ID],[CALL_ID],[DOMAIN_ID],[SENTIMENT_ID],[SENTIMENT_SCORE],[CREATED_DATE],[CREATED_BY],[UPDATED_DATE],[UPDATED_BY])VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)\",row['Result_Id'],row['CALL_ID'],row['DOMAIN_ID'],row['SENTIMENT_ID'],row['SENTIMENT_SCORE'],datetime.datetime.now(),row['CREATED_BY'],datetime.datetime.now(),row['CREATED_BY'])\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    logging.debug(\"Download preprocessed transcripts file from azure storage\")\n",
    "    try:\n",
    "        #download preprocessed data file from azure blob storage\n",
    "        local_file_name=get_preprocessed_file_from_azure()\n",
    "        logging.debug(\"Downloaded preprocessed transcripts file from azure storage\")\n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured while fetching file from azure storage :\"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logging.debug(\"Reading the downloaded preprocessed transcripts file into a dataframe\")\n",
    "    try:\n",
    "        #storing the downloaded file into a dataframe\n",
    "        preprocessed_data=pd.read_csv(local_file_name)\n",
    "        logging.debug(\"Dataframe is populated with preprocessed transcripts\")\n",
    "        \n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logging.debug(\"Fetching the sentiments and threshold from DB\")\n",
    "    try:\n",
    "        #Fetch the topics to be created from database\n",
    "        df_master_sentiments=get_sentiment_label_from_DB()\n",
    "        logging.debug(\"Sentiments and threshold fetched from DB\")\n",
    "    except Exception as err:\n",
    "        if \"Could not open a connection to SQL Server\" in str(err):\n",
    "            logging.error(\"Could not connect to data warehouse : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        else:\n",
    "            logging.error(\"Error occured with database :\"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logging.debug(\"Getting the sentiment score from azure cognitive service\")\n",
    "    try:\n",
    "        #storing the downloaded file into a dataframe\n",
    "        df_sentiment_Score=get_sentiment_score_from_Azure(preprocessed_data)\n",
    "        logging.debug(\"Sentiment Score fetched from azure cognitive service\")\n",
    "        \n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logging.debug(\"Getting the sentiment score from azure cognitive service\")\n",
    "    try:\n",
    "        #storing the downloaded file into a dataframe\n",
    "        df_sentiment_Score=get_sentiment_score_from_Azure(preprocessed_data)\n",
    "        logging.debug(\"Sentiment Score fetched from azure cognitive service\")\n",
    "        \n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    \n",
    "    logging.debug(\"Staging data to be inserted into database\")\n",
    "    try:\n",
    "        final_df = staging_final_dataframe(preprocessed_data,df_sentiment_Score,df_master_sentiments)\n",
    "        logging.debug(\"Final data frame is ready to be inserted into database\")\n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    logging.debug(\"Push data into database\")\n",
    "    try:\n",
    "        insert_data_into_database(final_df)\n",
    "        logging.debug(\"Data inserted into database\")\n",
    "    except Exception as err:\n",
    "        if \"Could not open a connection to SQL Server\" in str(err):\n",
    "            logging.error(\"Could not connect to data warehouse : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        else:\n",
    "            logging.error(\"Error occured with database :\"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        logging.debug(\"End of sentiment analysis\")\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
