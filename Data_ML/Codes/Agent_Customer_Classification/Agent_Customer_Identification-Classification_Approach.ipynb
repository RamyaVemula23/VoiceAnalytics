{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ConfigFile.ipynb  ##import ConfigFile as cfg\n",
    "%run DB_Connection.ipynb ##import DB_Connection as dbc\n",
    "import pyodbc\n",
    "import copy\n",
    "import math\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Text preprocessing imports\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Modelling and evaluation imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger SAAB (DEBUG)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a unique id with the help of current date timestamp\n",
    "st = datetime.fromtimestamp(time.time()).strftime('%d%m%Y%H%M%S')\n",
    "clientname=client_name['clientname']\n",
    "logging.basicConfig(filename=clientname+\"_\"+st+\".log\",level=logging.DEBUG,format=\"%(asctime)s:%(levelname)s:%(message)s\")\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "logging.getLogger(clientname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function preprocesses the transcript column to remove punctuation, whitespaces and to lowercase & lemmatize them \n",
    "    Input:\n",
    "        data : the entire dataframe on which the preprocessing needs to be performed\n",
    "    Output:\n",
    "        preprocessed list of transcripts\n",
    "'''\n",
    "def preprocess_text(input_dataframe,target_column_number):\n",
    "    # Preprocessing text\n",
    "    lemmatized_text = []\n",
    "    count = 0\n",
    "    for row in input_dataframe.itertuples():\n",
    "        # Removing extra spaces, special characters and punctuation marks and lower casing the text\n",
    "        clean_text = re.sub(r'\\s\\s+',r' ',re.sub(r'[?|$|.|!|#|%|^|*|:|;|,|+|-|_|=|&]',r'',row[target_column_number].lower()))\n",
    "        # Lemmatizing the text\n",
    "        lemmatized_text.append(\" \".join([str(token.lemma_).replace('-PRON-',str(token)) for token in nlp(str(clean_text))]))\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function performs a stratified sampling on the input data frame \n",
    "    Input:\n",
    "        input_dataframe : on which the sampling needs to be performed\n",
    "    Output:\n",
    "        Returns the train data, train labels, test data and test labels\n",
    "'''\n",
    "def stratified_sampling(input_dataframe):\n",
    "    X = input_dataframe['Lemmatized_Text']\n",
    "    y = input_dataframe['SpeakerId']\n",
    "    sss = StratifiedShuffleSplit(n_splits=10,test_size=0.2,random_state=32)\n",
    "    sss.get_n_splits(X, y)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "    return  X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function performs a featre selection based on the chi2 scores to exclude less weightage score\n",
    "    Input:\n",
    "        X_train : train data\n",
    "        y_train : train labels\n",
    "        no_of_features : \n",
    "        threshold = chi2 square threshold value\n",
    "    Output:\n",
    "        Dataframe of the best features\n",
    "'''\n",
    "def feature_selection(X_train,y_train,no_of_features,chi_square_threshold):\n",
    "    # Create dummies for classes\n",
    "    y_train_dummies = pd.get_dummies(y_train)\n",
    "    \n",
    "    # Convert to token counts\n",
    "    count_vec = CountVectorizer(ngram_range=(2,3),binary=True)\n",
    "    # count_vec = CountVectorizer(binary=True)\n",
    "    X_train_count = count_vec.fit_transform(X_train)\n",
    "    transformer = TfidfTransformer()\n",
    "    X_train_transformed = transformer.fit_transform(X_train_count)\n",
    "    \n",
    "    columns = list(y_train_dummies.columns)\n",
    "    features_final=[]\n",
    "    chi_probability = []\n",
    "    for col in columns:\n",
    "        chi_score = chi2(X_train_transformed, y_train_dummies[col])[0]\n",
    "        chi_prob = chi2(X_train_transformed, y_train_dummies[col])[1]\n",
    "        features = count_vec.get_feature_names()\n",
    "        chi_table = pd.DataFrame({'Features':features, 'Chisquare':chi_score , 'Chi_Square_Prob':chi_prob})\n",
    "        chi_table_cutoff = chi_table.loc[(chi_table[\"Chisquare\"] > chi_square_threshold)] \n",
    "        if len(chi_table_cutoff)>no_of_features:\n",
    "            chi_table = chi_table_cutoff.sort_values(by='Chisquare', ascending=False).head(no_of_features)\n",
    "        else:\n",
    "            chi_table = chi_table_cutoff\n",
    "        features_final.append(chi_table['Features'].tolist())\n",
    "        chi_probability.append(chi_table['Chisquare'].tolist())\n",
    "    features_all = [item for sublist in features_final for item in sublist]\n",
    "    chi_values_all = [item for sublist in chi_probability for item in sublist]\n",
    "    features_all_table = pd.DataFrame({'Features':features_all , 'Chisquare':chi_values_all})\n",
    "    features_best_chi = pd.DataFrame(features_all_table.groupby(['Features'], as_index=False, sort=False)['Chisquare'].max())\n",
    "    \n",
    "    return  features_best_chi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best k features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function identifies the best numebr features to be used\n",
    "    Input:\n",
    "        features_best_chi : best features by chi2 score\n",
    "        X_train : train data\n",
    "        y_train : train labels\n",
    "        X_test : test data\n",
    "        y_test : test labels\n",
    "        \n",
    "    Output:\n",
    "        Return the optimal number of features to consider\n",
    "'''\n",
    "def find_best_k_features(features_best_chi,X_train,y_train,X_test,y_test):\n",
    "    k_val = features_best_chi.shape[0]\n",
    "    scores_f1 = []\n",
    "    for k in range(1, int(k_val), 1):\n",
    "        features_table = features_best_chi.sort_values('Chisquare', ascending=False).head(k)\n",
    "        # Bi gram\n",
    "        model = Pipeline([('vect', CountVectorizer(vocabulary=features_table.Features,ngram_range=(2,3))),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf',  CalibratedClassifierCV(LinearSVC(penalty=\"l2\", dual=False, tol=1e-3),cv=KFold(n_splits=3))),])#LinearSVC(penalty=\"l2\", dual=False,tol=1e-3)\n",
    "        # Training the Model\n",
    "        model.fit(X_train, y_train)\n",
    "        # Scoring the Model\n",
    "        predicted = model.predict(X_test)\n",
    "        scores_f1.append(f1_score(y_test, predicted, average='weighted'))\n",
    "\n",
    "    kvals_list = list(range(1,int(k_val),1))\n",
    "\n",
    "    scores_f1_table = pd.DataFrame({'K_values':kvals_list, 'F1-Score':scores_f1})\n",
    "    kval_top = scores_f1_table.sort_values(by='F1-Score', ascending=False).head(n=10)\n",
    "    kval_optimum = int(kval_top.iloc[0,0])\n",
    "    \n",
    "    return kval_optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This function trains the final model\n",
    "    Input:\n",
    "        features_table_chi_top : top best features selected\n",
    "        X_train : train data\n",
    "        y_train : train labels\n",
    "        \n",
    "    Output:\n",
    "        Returns the trained model\n",
    "'''\n",
    "def train_model(features_table_chi_top,X_train,y_train):\n",
    "    #Using calibrated classifier \n",
    "    model = Pipeline([('vect', CountVectorizer(vocabulary=features_table_chi_top.Features,ngram_range=(2,3))),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf',  CalibratedClassifierCV(LinearSVC(penalty=\"l2\", dual=False,tol=1e-3),cv=KFold(n_splits=3)))])\n",
    "\n",
    "    # Training the Model\n",
    "    trained_model = model.fit(X_train, y_train)\n",
    "    return trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returns the count of agent labels for each of the speaker\n",
    "def get_speaker_A_count(section_spk_list,pred_labels_list):\n",
    "    labels = {}\n",
    "    spk_2_count = 0\n",
    "    spk_1_count = 0\n",
    "    for i in list(zip(section_spk_list,pred_labels_list)):\n",
    "        if ((i[0] == 2) and (i[1] == 'A')):\n",
    "            spk_2_count += 1\n",
    "        elif ((i[0]==1 and (i[1]=='A'))):\n",
    "            spk_1_count += 1\n",
    "    labels['Spk_2_A_Count'] = spk_2_count\n",
    "    labels['Spk_1_A_Count'] = spk_1_count\n",
    "    return labels\n",
    "\n",
    "# Function returns the count of customer labels for each of the speaker\n",
    "def get_speaker_C_count(section_spk_list,pred_labels_list):\n",
    "    labels = {}\n",
    "    spk_2_count = 0\n",
    "    spk_1_count = 0\n",
    "    for i in list(zip(section_spk_list,pred_labels_list)):\n",
    "        if ((i[0] == 2) and (i[1] == 'C')):\n",
    "            spk_2_count += 1\n",
    "        elif ((i[0]==1 and (i[1]=='C'))):\n",
    "            spk_1_count += 1\n",
    "    labels['Spk_2_C_Count'] = spk_2_count\n",
    "    labels['Spk_1_C_Count'] = spk_1_count\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Function that assigns the agent/customer label to the speaker ids by \n",
    "  evaluating the intro, closure and mid sections of the transcription\n",
    "  \n",
    "    Input :\n",
    "        agent_count_dict : Dictionary with counts of the speakers classified as Agent\n",
    "        cust_count_dict : Dictionary with counts of the speakers classified as Customer\n",
    "    Output :\n",
    "        Return the final speaker ids associated to both Agent and Customer\n",
    "'''\n",
    "def assign_speaker_by_section(agent_count_dict,cust_count_dict):\n",
    "    \n",
    "    agent_speaker = np.nan\n",
    "    cust_speaker = np.nan\n",
    "\n",
    "    if (((agent_count_dict['Spk_1_A_Count']) > (cust_count_dict['Spk_1_C_Count'])) and ((cust_count_dict['Spk_2_C_Count']) > (agent_count_dict['Spk_2_A_Count']))):\n",
    "        agent_speaker = 1\n",
    "        cust_speaker = 2\n",
    "    elif (((agent_count_dict['Spk_1_A_Count']) > (cust_count_dict['Spk_1_C_Count'])) and ((cust_count_dict['Spk_2_C_Count']) == (agent_count_dict['Spk_2_A_Count']))):\n",
    "        agent_speaker = 1\n",
    "        cust_speaker = 2\n",
    "    elif (((agent_count_dict['Spk_1_A_Count']) > (cust_count_dict['Spk_1_C_Count'])) and ((cust_count_dict['Spk_2_C_Count']) < (agent_count_dict['Spk_2_A_Count']))):\n",
    "        if (agent_count_dict['Spk_2_A_Count'] > agent_count_dict['Spk_1_A_Count']):\n",
    "            agent_speaker = 2\n",
    "            cust_speaker = 1\n",
    "        elif ((agent_count_dict['Spk_2_A_Count'] < agent_count_dict['Spk_1_A_Count'])):\n",
    "            agent_speaker = 1\n",
    "            cust_speaker = 2\n",
    "    elif (((agent_count_dict['Spk_1_A_Count']) == (cust_count_dict['Spk_1_C_Count'])) and ((cust_count_dict['Spk_2_C_Count']) > (agent_count_dict['Spk_2_A_Count']))):\n",
    "        agent_speaker = 1\n",
    "        cust_speaker = 2\n",
    "    elif (((agent_count_dict['Spk_1_A_Count']) == (cust_count_dict['Spk_1_C_Count'])) and ((cust_count_dict['Spk_2_C_Count']) == (agent_count_dict['Spk_2_A_Count']))):\n",
    "        pass\n",
    "    elif (((agent_count_dict['Spk_1_A_Count']) == (cust_count_dict['Spk_1_C_Count'])) and ((cust_count_dict['Spk_2_C_Count']) < (agent_count_dict['Spk_2_A_Count']))):\n",
    "#         print(\"this condition\")\n",
    "        agent_speaker = 2\n",
    "        cust_speaker = 1        \n",
    "    elif ((agent_count_dict['Spk_1_A_Count'] < cust_count_dict['Spk_1_C_Count']) and (cust_count_dict['Spk_2_C_Count'] > agent_count_dict['Spk_2_A_Count'])):\n",
    "        if (cust_count_dict['Spk_2_C_Count'] > cust_count_dict['Spk_1_C_Count']):\n",
    "            agent_speaker = 1\n",
    "            cust_speaker = 2\n",
    "        elif ((cust_count_dict['Spk_2_C_Count'] < cust_count_dict['Spk_1_C_Count'])):\n",
    "            agent_speaker = 2\n",
    "            cust_speaker = 1\n",
    "    elif (((agent_count_dict['Spk_1_A_Count']) < (cust_count_dict['Spk_1_C_Count'])) and ((cust_count_dict['Spk_2_C_Count']) == (agent_count_dict['Spk_2_A_Count']))):\n",
    "        agent_speaker = 2\n",
    "        cust_speaker = 1\n",
    "    elif (((agent_count_dict['Spk_1_A_Count']) < (cust_count_dict['Spk_1_C_Count'])) and ((cust_count_dict['Spk_2_C_Count']) < (agent_count_dict['Spk_2_A_Count']))):\n",
    "        agent_speaker = 2\n",
    "        cust_speaker = 1\n",
    "    return agent_speaker,cust_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Function predicts and assigns \n",
    "  \n",
    "    Input :\n",
    "        number_of_turns_considered : input parameter to define boundaries for intro,mid and closure section\n",
    "        azure_data : unseen data\n",
    "        trained_model : trained model\n",
    "    Output :\n",
    "        Returns dictonary of call ids and the respective agent label\n",
    "'''\n",
    "def get_call_agent_speaker_id(number_of_turns_considered,azure_data,azure_distict_call_ids,trained_model):\n",
    "    call_agent_ids = {}\n",
    "    for call_id in azure_distict_call_ids:\n",
    "\n",
    "        temp_data = azure_data[azure_data['Call_ID']==call_id][['Call_ID','SpeakerId','StartTime','Display','Lemmatized_Text']]\n",
    "        labels = trained_model.predict(temp_data['Lemmatized_Text'])    \n",
    "        # Assuming always will have a 9 or greater turns\n",
    "        if len(temp_data) >= 9:\n",
    "            agent_spk_id = np.nan\n",
    "            # Getting the speaker id counts for agent label in all the three sections\n",
    "            intro_A_counts = get_speaker_A_count(list(temp_data[:number_of_turns_considered].SpeakerId),list(labels[:number_of_turns_considered])) \n",
    "            closure_A_counts = get_speaker_A_count(list(temp_data[-(number_of_turns_considered):].SpeakerId),list(labels[-(number_of_turns_considered):])) \n",
    "            mid_A_counts = get_speaker_A_count(list(temp_data[number_of_turns_considered:-(number_of_turns_considered)].SpeakerId),list(labels[number_of_turns_considered:-(number_of_turns_considered)])) \n",
    "            # Getting the speaker id counts for customer label in all the three sections\n",
    "            intro_C_counts = get_speaker_C_count(list(temp_data[:number_of_turns_considered].SpeakerId),list(labels[:number_of_turns_considered])) \n",
    "            closure_C_counts = get_speaker_C_count(list(temp_data[-(number_of_turns_considered):].SpeakerId),list(labels[-(number_of_turns_considered):])) \n",
    "            mid_C_counts = get_speaker_C_count(list(temp_data[number_of_turns_considered:-(number_of_turns_considered)].SpeakerId),list(labels[number_of_turns_considered:-(number_of_turns_considered)])) \n",
    "\n",
    "            intro_agent_id , intro_cust_id = assign_speaker_by_section(intro_A_counts,intro_C_counts)\n",
    "            closure_agent_id , closure_cust_id = assign_speaker_by_section(closure_A_counts,closure_C_counts)\n",
    "\n",
    "            # First check intro and closure section for majority count of the speaker ids and then tie breaker using the mid section\n",
    "            if (not math.isnan(intro_agent_id) and not math.isnan(closure_agent_id)):\n",
    "                if intro_agent_id == closure_agent_id:\n",
    "                    agent_spk_id = intro_agent_id\n",
    "                else:\n",
    "                    mid_agent_id , mid_cust_id = assign_speaker_by_section(mid_A_counts,mid_C_counts)\n",
    "                    if not math.isnan(mid_agent_id):\n",
    "                        agent_spk_id = mid_agent_id\n",
    "                    else:\n",
    "                        agent_spk_id = intro_agent_id\n",
    "\n",
    "            elif not math.isnan(intro_agent_id):\n",
    "                agent_spk_id = intro_agent_id\n",
    "            elif not math.isnan(closure_agent_id):\n",
    "                agent_spk_id = closure_agent_id\n",
    "            else:\n",
    "                mid_agent_id , mid_cust_id = assign_speaker_by_section(mid_A_counts,mid_C_counts)\n",
    "                if not math.isnan(mid_agent_id):\n",
    "                    agent_spk_id = mid_agent_id\n",
    "\n",
    "        call_agent_ids[call_id] = agent_spk_id\n",
    "    return call_agent_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to DW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Function to write the dataframe to DW\n",
    "  \n",
    "    Input :\n",
    "        call_speaker_labels_df : dataframe that consists of speakr labels for each call\n",
    "            \n",
    "'''\n",
    "def write_df_to_dw(call_speaker_labels_df):\n",
    "    # Writing to DW\n",
    "    today_timestamp = datetime.now()\n",
    "    server = 'saab-server-resource.database.windows.net'\n",
    "    database = 'SAAB_DW_Resource'\n",
    "    username = 'saabadmin'\n",
    "    password = 'p@$$w0rd'\n",
    "    conn = pyodbc.connect('DRIVER={ODBC Driver 13 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password)\n",
    "    cursor = conn.cursor()\n",
    "    id_count = 0\n",
    "    for index,row in call_speaker_labels_df.iterrows():\n",
    "        id_count += 1\n",
    "        cursor.execute(\"INSERT INTO [dbo].[SAAB_ML_SPEAKER_MAPPING_FT]([RESULT_ID],[CALL_ID],[DOMAIN_ID],[SPEAKER_ID],[LABEL],[CREATED_DATE],[CREATED_BY]) values (?, ?, ?, ?, ?, ?, ?)\", id_count, row['Call_ID'],'101', row['SpeakerId'], row['Labels'], today_timestamp,'Mrinalini') \n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logging.debug(\"Starting....\")\n",
    "    #Reading data from local system and azure data\n",
    "    try:\n",
    "        manual_trasncript_data = pd.read_excel(\"EnglishManualTranscriptDF.xlsx\",sheet_name=\"Sheet3\")\n",
    "        azure_data = fetch_data(\"select * from [dbo].[Fact_Audio_Insights] order by [Call_ID],[StartTime];\")\n",
    "    except Exception as err:\n",
    "        if \"Could not open a connection to SQL Server\" in str(err):\n",
    "            logging.error(\"Could not connect to data warehouse : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        else:\n",
    "            logging.error(\"Error occured with database :\"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "        \n",
    "    try:\n",
    "        # Replacing a lowercase labels to an uppercase label\n",
    "        manual_trasncript_data.loc[manual_trasncript_data[manual_trasncript_data['SpeakerId']==\"c\"].index[0],'SpeakerId'] = 'C'\n",
    "\n",
    "        # Preprocess text\n",
    "        logging.debug(\"Preprocessing in progress....\")\n",
    "        lemmatized_text = preprocess_text(manual_trasncript_data,3)\n",
    "        manual_trasncript_data['Lemmatized_Text'] = lemmatized_text\n",
    "\n",
    "        #Stratified Sampling\n",
    "        logging.debug(\"Stratified sampling in progress....\")\n",
    "        X_train, y_train, X_test, y_test = stratified_sampling(manual_trasncript_data)\n",
    "\n",
    "        # Feature selection\n",
    "        logging.debug(\"Finding best features in progress....\")\n",
    "        features_best_chi = feature_selection(X_train,y_train,1000,1)\n",
    "\n",
    "        # Finding best K-features\n",
    "        kval_optimum = find_best_k_features(features_best_chi,X_train, y_train, X_test, y_test)\n",
    "\n",
    "        #Training data with optimal features\n",
    "        logging.debug(\"Training in progress....\")\n",
    "        features_table_chi_top = features_best_chi.sort_values('Chisquare', ascending=False).head(kval_optimum)\n",
    "        final_trained_model = train_model(features_table_chi_top,X_train, y_train)\n",
    "\n",
    "        # Predict on test\n",
    "        logging.debug(\"Prediction and scoring in progress....\")\n",
    "        predicted_labels = final_trained_model.predict(X_test)\n",
    "        prec_recall_fscore_support = precision_recall_fscore_support(y_test, predicted_labels)\n",
    "        accuracy = accuracy_score(y_test,predicted_labels)\n",
    "        \n",
    "        # Prediction on the Azure Transcripts - Unseen Data\n",
    "        logging.debug(\"Prediction on unseen data in progress....\")\n",
    "        azure_lemmatized_text = preprocess_text(azure_data,7)\n",
    "        azure_data['Lemmatized_Text'] = azure_lemmatized_text\n",
    "        azure_distict_call_ids = list(azure_data['Call_ID'].unique())\n",
    "\n",
    "        call_agent_ids = get_call_agent_speaker_id(5,azure_data,azure_distict_call_ids,final_trained_model)\n",
    "        copy_azure_data = copy.deepcopy(azure_data)\n",
    "        copy_azure_data['Agent_ID'] = copy_azure_data['Call_ID'].map(call_agent_ids)\n",
    "        copy_azure_data['Labels'] = np.where(copy_azure_data['SpeakerId'] == copy_azure_data['Agent_ID'],'A','C')\n",
    "        call_speaker_labels_df = copy_azure_data[['Call_ID','SpeakerId','Labels','StartTime']].groupby(['Call_ID','SpeakerId','Labels']).count().reset_index().drop('StartTime',axis=1)\n",
    "        \n",
    "        logging.debug(\"Writing to DW in progress....\")\n",
    "        write_df_to_dw(call_speaker_labels_df)\n",
    "        \n",
    "    except Exception as err:\n",
    "#         print(err.with_traceback)\n",
    "        logging.error(\"Error occured : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        logging.debug(\"End of identification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG - Starting....\n",
      "DEBUG - Preprocessing in progress....\n",
      "DEBUG - Stratified sampling in progress....\n",
      "DEBUG - Finding best features in progress....\n",
      "DEBUG - Training in progress....\n",
      "DEBUG - Prediction and scoring in progress....\n",
      "DEBUG - Prediction on unseen data in progress....\n",
      "DEBUG - Writing to DW in progress....\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
