{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ConfigFile as cfg\n",
    "import DB_Connection as dbc\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import numpy as np\n",
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import ContentSettings\n",
    "import os\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "'''from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')'''\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger SAAB (DEBUG)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a unique id with the help of current date timestamp\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%d%m%Y%H%M%S')\n",
    "clientname=cfg.client_name['clientname']\n",
    "logging.basicConfig(\n",
    "    filename=clientname+\"_\"+st+\".log\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s:%(levelname)s:%(message)s\"\n",
    ")\n",
    "logging.getLogger(clientname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting_transcripts_to_sentences(raw_transcripts):\n",
    "    #spliting the raw transcript into sentences.\n",
    "    gensim_text=[]\n",
    "    for transcripts in raw_transcripts['Raw Transcripts']:\n",
    "        gensim_text.append(list(gensim.summarization.textcleaner.get_sentences(str(transcripts).replace('\\r',' '))))\n",
    "    \n",
    "    #Appending the splitted sentences to the dataframe\n",
    "    raw_transcripts[\"Full_Sentences\"] = gensim_text\n",
    "    return raw_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing_splitted_transcripts(raw_transcripts_gensim):\n",
    "    #lemmatizing the splitted sentence transcript and removing the POS tagging for pronouns\n",
    "    test_list=[]\n",
    "    for full_text in raw_transcripts_gensim[\"Full_Sentences\"]:\n",
    "         test_list.append(\" \".join([str(token.lemma_).replace('-PRON-','') for token in nlp(str(full_text))]))\n",
    "\n",
    "    #Appending the lemmatized transcript to the dataframe\n",
    "    raw_transcripts_gensim[\"Lemmatized_transcript\"]=test_list\n",
    "    return raw_transcripts_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to remove gensim defined stopwords and new custom stop words from the transcript\n",
    "'''\n",
    "def preprocess(text):\n",
    "    newStopWords =cfg.custom_Stopwords['custom_stopwords']\n",
    "    result=[]\n",
    "    for token in text :\n",
    "        if token not in (gensim.parsing.preprocessing.STOPWORDS and  newStopWords) and len(token) > 3:\n",
    "            #print(token)\n",
    "            result.append((token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigrams_and_remove_Stopwords(raw_transcripts_gensim):\n",
    "    #creating phrases like good_afternoon, hdfc_life so that they can be removed as part of custom stop words.\n",
    "    Preprocessed_Transcripts=[]\n",
    "    bigrams=[]\n",
    "    import gensim, pprint\n",
    "    for transcripts in raw_transcripts_gensim[\"Lemmatized_transcript\"]:\n",
    "        tokens = [list(gensim.utils.tokenize(transcripts, lower=True))]\n",
    "\n",
    "        bigram_mdl = gensim.models.phrases.Phrases(tokens, min_count=1, threshold=5)\n",
    "        #Preprocessed_Transcripts.append(token_bigrams(transcripts))\n",
    "        from gensim.parsing.preprocessing import preprocess_string, remove_stopwords\n",
    "        CUSTOM_FILTERS = [remove_stopwords]\n",
    "        #if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3\n",
    "        tokens = [preprocess_string(\" \".join(word), CUSTOM_FILTERS) for word in tokens]\n",
    "        bigrams = bigram_mdl[tokens]\n",
    "        Preprocessed_Transcripts.append(list(bigrams))   \n",
    "    \n",
    "    #final step of preprocessing to remove stop words\n",
    "    Final_Preprocessed_Transcripts=[]\n",
    "    for tokenised_text in Preprocessed_Transcripts:\n",
    "        #print(tokenised_text)\n",
    "        for token in tokenised_text:\n",
    "            Final_Preprocessed_Transcripts.append(preprocess(token))\n",
    "    \n",
    "     #Appending the final preprocessed transcript to the dataframe\n",
    "    raw_transcripts_gensim[\"Preprocessed Transcripts\"]=Final_Preprocessed_Transcripts\n",
    "    \n",
    "    return raw_transcripts_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_file_to_azure(filename):\n",
    "    #fetching the blob account details, account key and container name from config file.\n",
    "    accountname=cfg.azure_details['azure_storage_name']\n",
    "    accountkey=cfg.azure_details['azure_storage_account_key']\n",
    "    containername=cfg.azure_details['storage_preprocessed_transcripts']\n",
    "    filepath=os.getcwd()\n",
    "    block_blob_service = BlockBlobService(account_name=accountname, account_key=accountkey)\n",
    "    #Upload the CSV file to Azure cloud\n",
    "    block_blob_service.create_blob_from_path(container_name= containername,\n",
    "                                             blob_name=filename,\n",
    "                                             file_path=filepath+'\\\\'+filename,content_settings=ContentSettings(content_type='application/CSV'))\n",
    "    \n",
    "    return 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def main():\n",
    "    logging.debug(\"Starting preprocessing\")\n",
    "    raw_transcripts = pd.DataFrame()\n",
    "    #Get the raw transcripts from the data warehouse\n",
    "    try:\n",
    "        raw_transcripts=dbc.fetch_data(\"exec dbo.[SP_GET_PROCESSED_CALL_TRANSCRIPTS]\")\n",
    "    except Exception as err:\n",
    "        if \"Could not open a connection to SQL Server\" in str(err):\n",
    "            logging.error(\"Could not connect to data warehouse : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        else:\n",
    "            logging.error(\"Error occured with database :\"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        #Get the transcripts splitted into meaningful sentences.\n",
    "        raw_transcripts=splitting_transcripts_to_sentences(raw_transcripts)\n",
    "        #Get lemmas for the splitted sentences\n",
    "        transcripts_df=lemmatizing_splitted_transcripts(raw_transcripts)\n",
    "        #Remove stop words from the lemmatized dataframe after bigrams are created\n",
    "        transcripts_df=create_bigrams_and_remove_Stopwords(transcripts_df) \n",
    "        \n",
    "        #Dropping the temporary columns which were created in the datafram\n",
    "        transcripts_df.drop(['Full_Sentences', 'Lemmatized_transcript'], axis=1, inplace=True)\n",
    "\n",
    "        #creating the filename with the unique id for the dataframe to be stored into\n",
    "        filename = \"Preprocessed_Transcripts_\" + str(st) + \".csv\"\n",
    "\n",
    "        #creating the file for the dataframe\n",
    "        raw_transcripts.to_csv(filename,index=False)\n",
    "        \n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured : \"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        status = move_file_to_azure(filename)\n",
    "    except Exception as err:\n",
    "        logging.error(\"Error occured while pushing file to azure storage :\"+str(err.args)+\"\\nTraceback :\"+str(err.with_traceback))\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        logging.debug(\"End of prepreocessing\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
