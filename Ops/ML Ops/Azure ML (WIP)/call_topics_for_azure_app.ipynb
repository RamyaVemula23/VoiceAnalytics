{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(req: func.HttpRequest) -> func.HttpResponse:\n",
    "    global model\n",
    "    logging.info('Python HTTP trigger function processed a request.')\n",
    "    logging.info(os.getcwd())\n",
    "    #modelpath = \"./httptopicmodeleval/\"\n",
    "    #file = open(\"Topic_Modelling.pkl\", 'rb')\n",
    "    model = joblib.load(\"/Topic_Modelling.pkl\")# pickle.load(file)\n",
    "    #file.close() \n",
    "    #loadModel(modelpath)\n",
    "    #logging.info(\"Model Loaded\" + str(model))\n",
    "    \n",
    "    ##############################################################################################\n",
    "    preprocessed_data = prepare_dataframe_from_json(raw_data)\n",
    "    df_master_topics = get_topics_from_DB()\n",
    "    Number_Of_Topics = len(df_master_topics)\n",
    "    bow_corpus = data_staging_for_prediction(preprocessed_data)\n",
    "    df_topics = predict_topics(bow_corpus,model,Number_Of_Topics)\n",
    "    final_df = staging_final_dataframe(preprocessed_data,df_topics,df_master_topics)\n",
    "    json_output=prepare_json_from_dataframe(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loadModel(modelpath):\n",
    "    return joblib.load(modelpath+\"Topic_Modelling.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Sanjeeb\\\\Project\\\\SAAB\\\\Pickle Files\\\\Call Topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-10f2f101a509>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"C:\\Sanjeeb\\Project\\SAAB\\Pickle Files\\Call Topics\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Sanjeeb\\Project\\SAAB\\Pickle Files\\Call Topics\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Sanjeeb\\\\Project\\\\SAAB\\\\Pickle Files\\\\Call Topics'"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0,\"C:\\Sanjeeb\\Project\\SAAB\\Pickle Files\\Call Topics\")\n",
    "model = joblib.load(open(r\"C:\\Sanjeeb\\Project\\SAAB\\Pickle Files\\Call Topics\", 'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"C:\\\\Sanjeeb\\\\Project\\\\SAAB\\\\Pickle Files\\\\Call Topics\\\\Topic_Modelling.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "import pickle\n",
    "import gensim\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_csv(\"Preprocessed_Transcripts_29012020161128.csv\")\n",
    "#raw_data = df[['Call_ID','Preprocessed Transcripts']].head(5).to_json(orient='records')#[1:-1].replace('},{', '} {')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_name = {'clientname' : 'SAAB'}\n",
    "mysql = {'server': 'saab-server-resource.database.windows.net',\n",
    "         'database': 'saab_dw_resource',\n",
    "         'username': 'saabadmin',\n",
    "         'password': 'p@$$w0rd',\n",
    "         'driver':'{ODBC Driver 17 for SQL Server}',\n",
    "         'PORT':'1433'}\n",
    "client_details = {'Domain_ID':'101','UserName':'stripathy32','call_language1':'en'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''preprocessed_data = prepare_dataframe_from_json(raw_data)\n",
    "df_master_topics = get_topics_from_DB()\n",
    "Number_Of_Topics = len(df_master_topics)\n",
    "bow_corpus = data_staging_for_prediction(preprocessed_data)\n",
    "df_topics = predict_topics(bow_corpus,model,Number_Of_Topics)\n",
    "final_df = staging_final_dataframe(preprocessed_data,df_topics,df_master_topics)\n",
    "json_output=prepare_json_from_dataframe(final_df)'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(query):\n",
    "    cnxn = pyodbc.connect('DRIVER='+mysql['driver']+';SERVER='+mysql['server']+';PORT='+mysql['PORT']+';DATABASE='+mysql['database']+';UID='+mysql['username']+';PWD='+ mysql['password'])\n",
    "\n",
    "    cursor = cnxn.cursor()\n",
    "    \n",
    "    #Stored procedure to fetch the call id and transcripts for the respective call id.\n",
    "    df_master_topics = pd.read_sql_query(query,cnxn)\n",
    "    \n",
    "    #Closing the connection\n",
    "    cursor.close()\n",
    "    cnxn.close()\n",
    "    return df_master_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_json_from_dataframe(final_df):\n",
    "    out = final_df.to_json(orient='records')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_from_json(raw_data):\n",
    "    df=pd.DataFrame(json.loads(raw_data))\n",
    "    preprocessed_data=df[['Call_ID','Preprocessed Transcripts']]\n",
    "    return preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_from_DB():\n",
    "    #df=pd.read_csv('Master.csv')\n",
    "    df=fetch_data(\"select calltopic_id,calltopics from [SAAB_ML_MASTER_CALLTOPICS_DM] where [DOMAIN_ID]=\"+client_details['Domain_ID'] +\"order by calltopic_id\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_staging_for_prediction(preprocessed_data):\n",
    "    vectorizer = CountVectorizer(ngram_range = (3,3)) \n",
    "    X1=vectorizer.fit_transform(preprocessed_data['Preprocessed Transcripts'])\n",
    "    trigrams=[]\n",
    "    for text in preprocessed_data['Preprocessed Transcripts']:\n",
    "        text=[text,]\n",
    "        X1=(vectorizer.fit_transform(text))\n",
    "        trigrams.append(vectorizer.get_feature_names()) \n",
    "\n",
    "    preprocessed_data['trigram_Transcript']=trigrams\n",
    "    dictionary = gensim.corpora.Dictionary(preprocessed_data['trigram_Transcript'])\n",
    "    \n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_data['trigram_Transcript']]\n",
    "    \n",
    "    return bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topics(bow_corpus,lda_model,num_topics):\n",
    "    #predicting on the corpus\n",
    "    doc_lda = lda_model[bow_corpus]\n",
    "    results=[]\n",
    "    for topic in doc_lda:\n",
    "        results.append(topic)\n",
    "    \n",
    "    #Getting the topics into a dataframe as column name and scores for each topic into cell per call\n",
    "    cols=[]\n",
    "    for i in range(0,num_topics):\n",
    "        cols.append(\"Topic_\"+str(i+1))\n",
    "    df_Topics=pd.DataFrame(columns=cols)\n",
    "    \n",
    "    for index in range(len(results)):\n",
    "        col_val=''\n",
    "        for item in (results[index]):\n",
    "            df_Topics.set_value(index, \"Topic_\"+str(item[0]+1), str(item[1]))\n",
    "    \n",
    "    return df_Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def staging_final_dataframe(preprocessed_data,df_Topics,df_master_topics):\n",
    "    df_Topics.columns=df_master_topics.calltopics\n",
    "    preprocessed_data=pd.concat([preprocessed_data, df_Topics], axis=1)\n",
    "    preprocessed_data.drop(['Preprocessed Transcripts'], axis=1, inplace=True)\n",
    "    value_vars=df_master_topics.calltopics.to_list()\n",
    "    df_result=pd.melt(preprocessed_data, id_vars =['Call_ID'], value_vars =value_vars ,\n",
    "              var_name ='CallTopic', value_name ='CALLTOPIC_SCORE').dropna().sort_values('Call_ID')\n",
    "    final_df=pd.merge(df_result, df_master_topics, how ='outer', left_on='CallTopic',right_on='calltopics')     \n",
    "    \n",
    "    final_df.sort_values(by=['Call_ID'],inplace=True)\n",
    "    final_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    final_df[\"Domain_ID\"]=int(client_details['Domain_ID'])\n",
    "    final_df['CREATED_BY']=client_details['UserName']\n",
    "    final_df['RESULT_ID']=final_df.index+1\n",
    "    final_df.drop(['CallTopic', 'calltopics'], axis=1, inplace=True)\n",
    "    \n",
    "    column_titles = ['RESULT_ID','Call_ID','Domain_ID','calltopic_id','CALLTOPIC_SCORE','CREATED_BY']\n",
    "    final_df.reindex(columns=column_titles)\n",
    "    \n",
    "    return final_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
